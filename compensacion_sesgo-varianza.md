<div style="text-align: justify">
 
 # Compensación de varianza y sesgo

Todo modelo predictivo está sujeto a algún tipo de error, que imposibilita obtener error nulo tanto en los sets de training como de testeo. Dependiendo de cómo esté construido el modelo, este puede ser propenso a cometer distintos tipos de error. En este caso, analizaremos el error por exceso de varianza y exceso de sesgo, y cómo buscar el mejor equilibrio entre ellos para obtener modelos más precisos. 

Para poder discutir estos conceptos, es necesario entender lo que es un **estimador**. De acuerdo con Goodfellow et al, _point estimation_ es “el intento de obtener la ‘mejor’ predicción de alguna cantidad de interés” (2016), de manera que un buen estimador será una función que entregue un valor estimado cercano al real. Por ejemplo, dada un grupo de **$M$** personas, se puede estimar la altura media **$h$** de este grupo tomando una muestra de menor tamaño **$m$** y calculando su altura media **$\hat{h}_m$**. El valor esperado de la función utilizada para encontrar el estimador puede o no ser igual al valor real del parametro que se está estimando; en caso de que ambos sean iguales, se dice que el estimador no tiene sesgo.  El *sesgo** o _bias_ de un estimador, entonces, se refiere a la diferencia entre el valor esperado del mismo y el valor real que se está estimando. En el ejemplo anterior, esto se sería **$bias(\hat{h}_m) = E(\hat{h}_m) - h$**. Un estimador se denomina no sesgado si su sesgo es igual a cero y sesgado si no lo es; también se puede decir que un estimador es asintóticamente no sesgado si su sesgo disminuye a medida que aumenta la cantidad de datos, de manera que **$\displaystyle \lim_{m \to \infty}bias(\hat{h}_m) = 0$**.

Siguiendo con nuestro ejemplo del estimador, si repitieramos el experimento con una nueva muestra de personas, es intuitivo que el promedio de sus estaturas será distinto al primer valor obtenido, a pesar de que estas personas pertenecen a la misma población. De hecho, lo esperable es que cada vez que se resamplee el valor del estimador obtenido sea un poco distinto; este cambio es un reflejo de la varianza del estimador. La **varianza** de los estimadores es una propiedad muy importante, que nos indica cómo esperamos que varíe el estimador al resamplear de forma independiente. Dada una muestra de datos de tamaño **$m$**, su varianza se calcula como $Var(f(x))=\frac{\Sigma (x_i - \mu)^2}{m-1}=E(X^2)-E(X)^2$, en que **$\mu$** es el promedio de la población. La varianza es tan importante porque una varianza alta implica que pequeños cambios en el input de un modelo o función pueden significar cambios muy grandes en su output, lo que generalmente no es deseable, además de que esta función será mucho más vulnerable frente a _outliers_ y datos erroneos.

En el contexto de _machine learning_, la varianza y el sesgo de un modelo están relacionadas mediante el concepto de **capacidad**. Goodfellow et al definen capacidad como la abiidad de un modelo de ajustarse a una amplia variedad de funciones, lo que en un principio puede parecer ambiguo. 

</div>
